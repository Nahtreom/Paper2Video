接下来，我们介绍一种用于Transformer模型的重要机制：多头注意力。首先，我们将输入的查询、键和值分别复制并投影到多个不同的注意力头中，在每个头内并行地进行注意力计算。然后，把这些头的输出拼接在一起，再进行一次线性投影，从而得到最终的输出表示。多头机制的作用就是让模型能同时关注到不同位置、不同子空间的信息，避免了单一注意力头所导致的信息平均化问题。