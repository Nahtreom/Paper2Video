接下来我们来看一下多头注意力机制，它将输入分别投影成多个子空间，通过并行计算多个注意力头来提取信息，每个头使用不同的矩阵参数W进行投影，这里我们采用了8个并行头，每个头的维度是64，这种方式不仅增强了模型捕捉信息的多样性，而且总计算量仍然和单头注意力机制保持差不多。