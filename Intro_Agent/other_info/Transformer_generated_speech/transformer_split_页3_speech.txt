现在我们看到的是Transformer里的解码器结构，它由六个一致的层堆叠而成，每一层都包含三个功能部分：首先是带有掩码的多头自注意力机制，确保预测时看不到之后的位置；然后是关注编码器输出的多头注意力；最后则是前馈网络。同时，每个子层间还有残差连接和层归一化，进一步提升模型的稳定性和性能。