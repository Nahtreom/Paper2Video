# é¡µ 8
Evaluating responses with ground-truth answers. To address this, MASLab adopts a unified evaluation framework centered on LLM-based evaluation methods grounded in ground-truth answers, designed to assess semantic correctness rather than superficial formatting. We support two primary variants: (1) A two-step pipeline using general-purpose LLMs, which first extracts a final answer from the MAS-generated output based on the query, and then compares it against the ground-truth to determine correctness; (2) A direct scoring approach using task-specific evaluators (e.g., xVerify [54]), which are fine-tuned to assess correctness across various domains. In addition, MASLab includes three commonly used rule-based evaluation strategies from the MAS literature.