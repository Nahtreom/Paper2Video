注意力机制主要分为两种，一种是加性注意力，它利用前馈神经网络计算，直观易懂但速度较慢；另一种是点积注意力，通过矩阵乘法实现，速度更快且空间效率更高，但当维度较大时，未缩放的点积会导致softmax函数饱和，梯度消失。因此，我们通常会对点积结果除以根号dk进行缩放，从而确保模型更稳定。