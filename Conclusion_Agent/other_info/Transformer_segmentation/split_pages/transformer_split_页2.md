# é¡µ 2
Encoder: The encoder is composed of a stack of  $N = 6$  identical layers. Each layer has two sub- layers. The first is a multi- head self- attention mechanism, and the second is a simple, positionwise fully connected feed- forward network. We employ a residual connection [11] around each of the two sub- layers, followed by layer normalization [1]. That is, the output of each sub- layer is  $\mathrm{LayerNorm}(x + \mathrm{Sublayer}(x))$ , where  $\mathrm{Sublayer}(x)$  is the function implemented by the sub- layer itself. To facilitate these residual connections, all sub- layers in the model, as well as the embedding layers, produce outputs of dimension  $d_{\mathrm{model}} = 512$ .